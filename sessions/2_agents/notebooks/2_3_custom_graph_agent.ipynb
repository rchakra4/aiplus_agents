{"cells":[{"cell_type":"markdown","metadata":{"id":"9HVqDs3At78R"},"source":["# Custom Graph Agent\n","\n","This notebook demonstrates how to build a custom graph-based agent using LangGraph, showcasing advanced workflow control and state management, which we will use for multi-agent systems.\n","\n","This notebook is divided into four main sections:\n","\n","1. **Setup and Configuration**: \n","   - Installation of required packages (langchain-openai, langchain-community, langgraph)\n","   - Configuration of OpenAI and Tavily API keys\n","   - Import of necessary typing and LangChain components\n","   - Langchain API key for tracing / observability\n","\n","2. **Graph Construction**: \n","   - Creation of a custom workflow using StateGraph\n","   - Visualize the graph using IPython display\n","   - Implementation of tool nodes and decision routing\n","   - Definition of model interaction functions\n","   - Setup of conditional edges for workflow control\n","   - Thread-based conversation tracking\n","\n","3. **Interactive Implementation**:\n","   - Interactive chat loop"]},{"cell_type":"markdown","metadata":{"id":"s_rVuJyQPRW7"},"source":["## Setup and Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28927,"status":"ok","timestamp":1730641070665,"user":{"displayName":"Nihar Reddy Kondam","userId":"16040040032885228293"},"user_tz":300},"id":"LiAD0uaMugQI","outputId":"14ba0d12-b7e8-4827-b45e-4f14e3ef758c"},"outputs":[],"source":["!pip install langchain-openai==0.2.0\n","!pip install langchain-community==0.3.1\n","!pip install langgraph==0.2.23"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25707,"status":"ok","timestamp":1730641096336,"user":{"displayName":"Nihar Reddy Kondam","userId":"16040040032885228293"},"user_tz":300},"id":"P4fGK1ZDPbSC","outputId":"b1b3e504-90fe-4cea-c9a2-7a1e82c589a3"},"outputs":[],"source":["import getpass\n","import os\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAi Key here:\")\n","os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily Key here:\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# langchain key for langsmith tracing / observability\n","os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your Langchain Key here:\")\n","os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n","os.environ[\"LANGCHAIN_PROJECT\"] = \"ODSC_Workshop\""]},{"cell_type":"markdown","metadata":{},"source":["## Graph Construction"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7551,"status":"ok","timestamp":1730641103881,"user":{"displayName":"Nihar Reddy Kondam","userId":"16040040032885228293"},"user_tz":300},"id":"A34dV20Ot78W"},"outputs":[],"source":["from typing import Annotated, Literal, TypedDict\n","from langchain_core.messages import HumanMessage\n","from langchain_openai import ChatOpenAI\n","from langchain_core.tools import tool\n","from langchain_community.tools.tavily_search import TavilySearchResults\n","from langgraph.checkpoint.memory import MemorySaver\n","from langgraph.graph import END, START, StateGraph, MessagesState\n","from langgraph.prebuilt import ToolNode"]},{"cell_type":"markdown","metadata":{"id":"4rg_yCBdPRW9"},"source":["- The code initializes a search tool with TavilySearchResults limiting to 2 results, and creates a ChatOpenAI instance with zero temperature for consistent outputs\n","\n","- `tool_node = ToolNode(tools)` creates a node that can handle the execution of the defined tools in the workflow\n","\n","- `should_continue` function acts as a router - it examines the last message in the state and decides whether to use tools or end the conversation\n","\n","- `call_model` function handles the actual interaction with the language model, taking the current state's messages and returning a new message list\n","\n","- The `StateGraph` initialization creates a workflow framework that maintains conversation state between interactions\n","\n","- Two main nodes are added to the workflow:\n"," - \"agent\": handles model interactions through call_model\n"," - \"tools\": manages external tool operations through tool_node\n","\n","- The workflow is structured with specific edges:\n"," - START → agent: Initial entry point\n"," - agent → conditional routing (tools or END)\n"," - tools → agent: Returns control to agent after tool use\n","\n","- `MemorySaver` is used to persist state between different runs of the graph, maintaining conversation context\n","\n","- The graph is compiled into a LangChain Runnable, making it executable with standard LangChain interfaces\n","\n","- The final execution shows how to invoke the workflow with an initial human message, using a specific thread_id for tracking"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"elapsed":5764,"status":"ok","timestamp":1730641109639,"user":{"displayName":"Nihar Reddy Kondam","userId":"16040040032885228293"},"user_tz":300},"id":"DYdHNP3pt78W","outputId":"8b40d98c-fa1d-465a-c8b8-03c1f0b8f4fb"},"outputs":[],"source":["tool = TavilySearchResults(max_results=2)\n","tools= [tool]\n","\n","tool_node = ToolNode(tools)\n","\n","model = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)\n","\n","# Define the function that determines whether to continue or not\n","def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n","    messages = state['messages']\n","    last_message = messages[-1]\n","    # If the LLM makes a tool call, then we route to the \"tools\" node\n","    if last_message.tool_calls:\n","        return \"tools\"\n","    # Otherwise, we stop (reply to the user)\n","    return END\n","\n","\n","# Define the function that calls the model\n","def call_model(state: MessagesState):\n","    messages = state['messages']\n","    response = model.invoke(messages)\n","    # We return a list, because this will get added to the existing list\n","    return {\"messages\": [response]}\n","\n","\n","# Define a new graph\n","workflow = StateGraph(MessagesState)\n","\n","# Define the two nodes we will cycle between\n","workflow.add_node(\"agent\", call_model)\n","workflow.add_node(\"tools\", tool_node)\n","\n","# Set the entrypoint as `agent`\n","# This means that this node is the first one called\n","workflow.add_edge(START, \"agent\")\n","\n","# We now add a conditional edge\n","workflow.add_conditional_edges(\n","    # First, we define the start node. We use `agent`.\n","    # This means these are the edges taken after the `agent` node is called.\n","    \"agent\",\n","    # Next, we pass in the function that will determine which node is called next.\n","    should_continue,\n",")\n","\n","# We now add a normal edge from `tools` to `agent`.\n","# This means that after `tools` is called, `agent` node is called next.\n","workflow.add_edge(\"tools\", 'agent')\n","\n","# Initialize memory to persist state between graph runs\n","checkpointer = MemorySaver()\n","\n","# Finally, we compile it!\n","# This compiles it into a LangChain Runnable,\n","# meaning you can use it as you would any other runnable.\n","# Note that we're (optionally) passing the memory when compiling the graph\n","app = workflow.compile(checkpointer=checkpointer)\n","\n","# Use the Runnable\n","final_state = app.invoke(\n","    {\"messages\": [HumanMessage(content=\"What is the buy / sell recommendation for NVIDIA stock on December 4 2024.\")]},\n","    config={\"configurable\": {\"thread_id\": 42}}\n",")\n","final_state[\"messages\"][-1].content"]},{"cell_type":"markdown","metadata":{},"source":["Displaying the graph using ipython display."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from IPython.display import Image, display\n","\n","try:\n","    display(Image(app.get_graph().draw_mermaid_png()))\n","except Exception as e:\n","    print(e)"]},{"cell_type":"markdown","metadata":{"id":"fcUiuxO4PRW-"},"source":["**State Management and Thread Continuity**\n","\n","- This code demonstrates conversation persistence by reusing the same thread_id (42), allowing the agent to maintain context from previous interactions\n","\n","- The app.invoke call introduces a new question about AMD while retaining the context from the previous NVIDIA query\n","\n","- By using the same thread_id in the config, the MemorySaver ensures all previous messages and context are available for this new interaction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"elapsed":5595,"status":"ok","timestamp":1730641115232,"user":{"displayName":"Nihar Reddy Kondam","userId":"16040040032885228293"},"user_tz":300},"id":"v-GtceREt78X","outputId":"0747c78f-f98a-4f5c-9945-8d0596a8a690"},"outputs":[],"source":["# Now when we pass the same \"thread_id\", the conversation context is retained\n","# via the saved state (i.e. stored list of messages)\n","\n","final_state = app.invoke(\n","    {\"messages\": [HumanMessage(content=\"What about AMD?\")]},\n","    config={\"configurable\": {\"thread_id\": 42}}\n",")\n","final_state[\"messages\"][-1].content"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1730641115232,"user":{"displayName":"Nihar Reddy Kondam","userId":"16040040032885228293"},"user_tz":300},"id":"CZhkCzAgt78X"},"outputs":[],"source":["config = {\"configurable\": {\"thread_id\": 42}}"]},{"cell_type":"markdown","metadata":{},"source":["## Loop the agent for interactive chat"]},{"cell_type":"markdown","metadata":{"id":"ZK5LEXrJPRW-"},"source":["**Interactive Chat Loop Implementation**\n","\n","- Sets up an infinite loop that continuously accepts user input until specific exit commands (\"quit\", \"exit\", \"q\") are given\n","\n","- Each user input is processed through the app.invoke method, maintaining the conversation state using the previously defined configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":87629,"status":"ok","timestamp":1730641406183,"user":{"displayName":"Nihar Reddy Kondam","userId":"16040040032885228293"},"user_tz":300},"id":"ZUVNzmvxt78X","outputId":"436f9dfe-3b20-454d-eef8-52efad161a7a"},"outputs":[],"source":["from pprint import pprint\n","while True:\n","    user_input = input(\"User: \")\n","    # print(\"User: \" + user_input)\n","    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n","        print(\"Assistant: Goodbye!\")\n","        break\n","    final_state = app.invoke(\n","        {\"messages\": [HumanMessage(content=user_input)]},\n","        config=config\n","    )\n","    print()\n","    pprint(final_state)\n","    print()\n","    print(\"Assistant: \" + final_state[\"messages\"][-1].content)\n","    print()"]},{"cell_type":"markdown","metadata":{},"source":["### Practice Exercise 2\n","\n","Replace the web search tool with RAG capabilities (using example from 3_1_multi_agent_system.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 1. Copy over RAG code from 3_1_multi_agent_system.ipynb\n","\n","# Step 2. Replace the web search tool with RAG tool\n","\n","# Step 3. Test the new pipeline with a comparative analysis between two stocks of your choice."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
